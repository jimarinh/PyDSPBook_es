{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apéndice C. Relaciones útiles del álgebra lineal para filtrado y optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la representación matemática de filtros digitales y para la descripción de algoritmos, principalmente los de filtrado adaptativo, es muy útil emplear representaciones y operaciones vectoriales que se muestran a continuación:\n",
    "\n",
    "1. Sea el vector columna $\\mathbf{x}[n]=\\left[x[n] x[n-1] x[n-2] ... x[n-Q+1]\\right]^H$, es decir, un vector que almacena $Q$ muestras del pasado de una señal. En esta notación, la $H$ denota la transpuesta del complejo conjugado. Si el vector $x$ es completamente real, se puede sustituir la $H$ por $T$, para denotar simplemente transpuesta. Un filtro FIR se puede expresar vectorialmente como $$y[n]=\\mathbf{w}^H\\mathbf{x}[n]$$ donde $\\mathbf{w}$ es el vector con los coeficientes del filtro.\n",
    "\n",
    "2. El error cuadrático medio se define como $$MSE=\\mathcal{E}\\left\\{ |e[n]|^2 \\right\\} = \\mathcal{E}\\left\\{e[n] e^* [n] \\right\\}$$\n",
    "\n",
    "3. La autocorrelación de una señal está dada por $$r_{x}[l]=\\mathcal{E}\\left\\{ x[n] x^{*}[n+l] \\right\\}$$\n",
    "\n",
    "4. Si se tiene un vector aleatorio $\\mathbf{x}$ (vector columna) se puede definir la matriz de autocorrelación como $$R_{x}=\\mathcal{E}\\left\\{\\mathbf{x} \\mathbf{x}^{H} \\right\\}$$ \n",
    "\n",
    "5. En el caso de tener un vector aleatorio $\\mathbf{x}$ y una variable aleatoria $d$, se define la correlación cruzada como: $$r_{d}=\\mathcal{E}\\left\\{ \\mathbf{x} d^{*} \\right\\}$$  \n",
    "\n",
    "6. En muchas aplicaciones de filtrado óptimo se busca el conjunto de coeficientes que minimicen el MSE, para ello son útiles las siguientes derivadas:\n",
    "\n",
    "$$\\frac{\\partial(\\mathbf{x})}{\\mathbf{x}}=\\mathbf{I}$$\n",
    "\n",
    "$$\\frac{\\partial(\\mathbf{A}\\mathbf{x})}{\\mathbf{x}}=\\mathbf{A}$$\n",
    "\n",
    "$$\\frac{\\partial(\\mathbf{A}\\mathbf{x})}{\\mathbf{x}^{H}}=\\mathbf{A}^{H}$$\n",
    "\n",
    "$$\\frac{\\partial(\\mathbf{x}^{H}\\mathbf{A})}{\\mathbf{x}}=\\mathbf{A}^{H}$$\n",
    "\n",
    "$$\\frac{\\partial(\\mathbf{x}^{H}\\mathbf{A})}{\\mathbf{x}^{H}}=\\mathbf{A}$$\n",
    "\n",
    "$$\\frac{\\partial(\\mathbf{x}^{H}\\mathbf{Ax})}{\\mathbf{x}}=\\mathbf{x}^{H}(\\mathbf{A}+\\mathbf{A}^{H})$$\n",
    "\n",
    "$$\\frac{\\partial(\\mathbf{x}^{H}\\mathbf{Ax})}{\\mathbf{x}^{H}}=(\\mathbf{A}+\\mathbf{A}^{H})\\mathbf{x}$$\n",
    "\n",
    "donde $\\mathbf{I}$ es la matriz identidad.\n",
    "\n",
    "7. Una matriz $\\mathbf{A}$ se puede descomponer en sus autovalores y autovectores de la forma $$\\mathbf{A}=\\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{H}$$ donde $\\mathbf{\\Lambda}$ es una matriz diagonal cuyos elementos de la diagonal son los autovalores de la matriz $\\mathbf{A}$ y las filas de la matriz $\\mathbf{V}$ son los autovectores asociados a los autovalores.\n",
    "\n",
    "Se debe recordar que los autovalores y los autovectores cumplen la propiedad que $\\mathbf{Av}=\\lambda \\mathbf{v}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
